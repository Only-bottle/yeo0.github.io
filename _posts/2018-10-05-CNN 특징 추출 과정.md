---
layout: post
title:  "CNN 특징 추출 과정"
subtitle:   "1"
categories: data
tags: image
comments: true






---



### CNN (Convolution Nerual Network) 특징 추출 과정

- 여러 블로그를 참고하면서 초보자도 이해하기 쉽게 정리해보았습니다.
- CNN의 특징 추출과정을 정리한 것이며 이어서 다음 게시물로 코드와 함께 CNN 구현과정을 정리할 예정입니다.

<br/>

##### [1. 개요](#개요)

##### [2. Convolution Layer](#convolution_layer)

##### [3. Pooling](#pooling)

#### **[4.Fully Connected Layer](#fully)**

<br/>

<br/>

<br/>

<br/>

---

### [1. 개요](#개요)

간단하게 설명하면 **CNN**은 기본적인 **Neural Network 앞에 여러 계층의 Convolutional Layer을 붙인 형태**와 같습니다.

<br/>

![text](https://t1.daumcdn.net/cfile/tistory/213C6141583ED6AB0A "출처:https://t1.daumcdn.net/cfile/tistory/213C6141583ED6AB0A")

<br/>

위의 *Convolution Layer* 부분이 아래 그림의 <u>Features extraction</u> 부분이 되고, *Neural Network* 부분이 아래 그림의 <u>Classification</u> 부분이 됩니다.



![text](https://taewanmerepo.github.io/2018/01/cnn/head.png "https://taewanmerepo.github.io/2018/01/cnn/head.png")

<br/>

<br/>

- ##### 특징

  - 이미지의 공간정보를 유지하면서 특징을 효과적으로 인식/분류
  - 학습 파라미터가 적음 ( Filter가 공유 파라미터로 사용되기 때문에)

<br/>

Convolution Layer 와 Pooling Layer을 여러 곂 쌓아 특징을 충분히 추출한 후 이를 배열 형태로 만드는 Flatten layer을 통해 최종적으로 배열 형태의 Fully Connected Layer을 만들어 분류모델에 적용하게 됩니다.

<br/>

<br/>

### [2. Convolution Layer](#convolution_layer)

 **합성곱**(Convolution), **채널**(Channel), **필터**(Filter)&Stride, **패딩** (Padding)의 과정을 거쳐 만들어 진 Layer를 Convolution Layer라고 부릅니다.



![text](https://taewanmerepo.github.io/2018/01/cnn/conv2.png "출처:https://taewanmerepo.github.io/2018/01/cnn/conv2.png")

<br/>

다음과 같은 과정을 거쳐 만들어진 Feature Map에 **활성함수**(Activation Function)을 적용한 **Activation Map**이 <u>Convolution Layer의 최종 출력 결과</u>가 됩니다.

<br/>

<br/>

#### 2-1. 채널 (Channel)

하나의 **칼라이미지**는 보통 RGB 3가지의 Channel로 이루어 진 **3차원 데이터**로 볼 수 있습니다. (흑백이미지는 1가지의 Channel로 구성된 2차원 데이터 ) 예를 들어 높이 39픽셀, 폭 31픽셀의 칼라이미지의 shape = (39,31,3) 이고, 같은 사이즈의 흑백이미지의 shape=(39,31,1)이 됩니다. 

[![image](https://taewanmerepo.github.io/2018/01/cnn/channel.jpg)](#map)

<br/>

<br/>

#### 2-2. 필터 (Filter) & Stride

**필터**는 이미지의 특징을 추출하기 위한 행렬입니다. 일반적으로 3\*3 또는 4\*4 의 정사각행렬로 정의가 되며 <u>데이터에 특징이 있으면 큰 값, 특징이 없다면 0에 가까운 값</u>으로 나오게 됩니다. 특징을 잘 찾을 수 있게 어떤 값의 행렬(필터)이 필요한지 찾는 과정이 CNN의 학습과정이라고 볼 수 있습니다.

<br/>

**Stride**는 **필터가 순회하는 간격**입니다. Stride=1이면 한칸씩 이동하며 Input data와 Filter의 합성곱을 수행하게 되는 식입니다. 

![image](https://taewanmerepo.github.io/2018/01/cnn/conv.png)

<br/>

위의 과정을 다음과 같이 반복합니다.

![image](https://taewanmerepo.github.io/2018/01/cnn/filter.jpg)

3개로 나눠진 채널(흑백의 경우는 1개) 별로 합성곱을 진행을 하여 Feature Map을 만들고 이들을 각 지정된 Stride만큼 이동시키면서 반복 수행해 Feature Map 행렬을 만들게 됩니다. 최종 Feature Map은 **각 채널별로 나온 Feature Map을 합한 것**이 됩니다.

위의 [그림](#map) 과 같게 됩니다.

<br/>

<br/>

#### 2-3. 패딩 (Padding)

Feature map의 크기는 원래의  Input data의 크기보다 작습니다. 이대로 학습을 수행시키게 되면 최종적인 Convolution Layer의 Output은 처음 Input보다 사이즈가 작게 되고, 이를 반복수행하면 결국 Neural Network을 적용시키기도 전에 많은 데이터가 유실될 것입니다. 충분한 특징값이 추출되기도 전에 결과값이 유실되게 되는 것이죠. 따라서  Convolution Layer의 Output size를 Input data size와 같게 하기 위해 데이터의 외곽에 특정 값(주로 0)으로 채우는 과정을 거치고 이를 패딩(Padding)이라 합니다.

![image](https://taewanmerepo.github.io/2018/01/cnn/padding.png)

Padding은 **사이즈조절** 기능 외에도 원본 데이터에 계속 0이라는 노이즈를 섞음으로서 **Overfitting을 방지**하는 효과가 있습니다. 

<br/>

<br/>

#### 2-4. 활성함수 (Activation function)

패딩까지 거친 데이터는 데이터가 가지고 있는 특징을 매우 큰값으로, 특징이 없는부분은 0에 가까운 값으로 나타내고 있습니다. 우리에겐 값의 크기가  중요한 것이 아니라 특징이 ''있다", "없다"가 중요하기 때문에 이를 바꿔주는 작업이 필요합니다. 이를 Activation function이 수행합니다.

![img](https://t1.daumcdn.net/cfile/tistory/227DB641583ED6AE3B)

CNN에선 다양한 Activation function중에 ReLU를 주로 사용하는데 주로 알려진 Sigmoid는 학습 중 Back Propagation 과정에서 값이 희석되는 현상이 발생하기 때문입니다.

Activation function을 적용한 데이터를 Activation map 이라 하며 이것이 **최종적인 Convolution layer의 Output**이 됩니다.

<br/><br/>

<br/>

### [3. 풀링 (Pooling) (=Sub Sampling)](#pooling)

풀링 레이어는 Convolution layer의 Output을 Input으로 사용하며, Activation Map의 **크기를 줄이거나 특정 부분을 강**조하는 용도로 사용됩니다. 어느정도 특징이 추출되었으면 모든 특징을 다 사용하는 것이 아니라 특징 중에서 특출난 것으로만 사용하겠다는 아이디어에서 나온 것 인데요. 주로 사용되는 방법은 Max pooling으로 정사각 행렬의 특정 영역 안의 값의 최대값을 그 정사각 행렬의 대푯값으로 사용하게 됩니다. 여기서 사용되는 정사각 행렬은 일반적으로 Stride로 설정했던 크기와 동일합니다.

![img](https://t1.daumcdn.net/cfile/tistory/2121E641583ED6AF23)

<br/>

풀링은 매번 Activation Map 에 적용되는 것이 아니라 데이터의 크기를 줄이고 싶을 때 **선택적으로 사용**합니다. 사용하게 된다면 적은 Datasize로 **효율적인 학습**이 가능하고 데이터의 크기를 줄이면서 임의적인 소실이 발생하기 때문에 Overfitting을 방지할 수 있다는 장점이 있습니다.

<br/>

<br/>

#### [4. Fully Connected Layer](#fully)

선택적 Pooling을 거쳐 만들어진 최종적인 Convolution layer를 Neural Network의 Input으로 사용하기 위해 행렬이 아닌 **배열로 만들어주는 과정**이 필요합니다. Flatten Layer가 그 역할을 합니다. 이 과정까지 거치면 처음 Input에서 특징만 추출한 최종적인 Layer가 완성됩니다. 이를 Neural Network에 적용해 최종적으로 우리가 원하는 분류를 수행하게 됩니다.

<br/>

<br/>

<br/>

<br/>

#### 출처 및 참고자료

http://taewan.kim/post/cnn/

http://bcho.tistory.com/1149

이 두 자료를 주로 보며 이해하고 정리했습니다. 너무 잘 설명되어 있으니 한번 보셔도 좋을 것 같습니다:)

