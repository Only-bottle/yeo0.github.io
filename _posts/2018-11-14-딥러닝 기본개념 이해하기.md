---
layout: post
title:  "딥러닝 기본개념 이해하기"
subtitle:   "3"
categories: data
tags: dl
comments: true


---

## 딥러닝 기본개념 이해하기

- 딥러닝에 기본적인 개념들을 간단히 짚고 넘어 가려고 한다.
- 깊은 내용을 다루기 보단 훑는 포스팅이므로 용도에 맞게 참고하면 좋을 것 같다.

<br/>

### 퍼셉트론 (Perceptron)

​![img](http://ataspinar.com/wp-content/uploads/2016/11/perceptron_schematic_overview.png)

- 다음과 같은 그림을 퍼셉트론이라고 한다.
- 여러개의 Input에 Weight sum을 계산해 o=Output을 생성한다. Weight는 Training과정에서 결정되며, Train data를 기반으로 생성한다. 이 과정을 Training이라고 한다. 
- Output은 Activation function에 의한 변환을 거쳐 출력된다.

<br/>

### 활성화 함수(Activation function)

- 신경망을 비선형으로 만들어 주는 함수이다. 
- Training과정에서의 함수는 gradient를 조정하는데 중요한 역할을 한다.

<br/>

#### - 시그모이드 함수 (Sigmoid)

![img](https://t1.daumcdn.net/cfile/tistory/2114E54358D60E4912)

![img](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png)

- 미분이 가능한 함수로 모든 값을 확률로 변환시키는데 유용하다. 
- Input 값에 따라서 0에서 1사이의 Output을 낸다.

- X값에 대한 Y값이 변화는 작다. 따라서 gradient가 소실될 가능성이 높다.
- 항상 신호를 출력하기 때문에 인공신경망이 무거워진다.

<br/>

#### - 쌍곡탄젠트함수 (tanh)

![img](https://www.medcalc.org/manual/_help/functions/tanh.png)

- sigmoid 함수의 scale된 버전이다. 범위가 Y의 범위가 [-1,1]인 것을 볼 수 있다.
- gradient는 sigmoid에 비해 안정적이므로 gradient 소실 문제가 발생할 가능성이 낮다. 
- 항상 신호를 출력하기 때문에 인공신경망이 무거워진다.

<br/>

#### - ReRU 함수

![img](https://cdn-images-1.medium.com/max/357/1*oePAhrm74RNnNEolprmTaQ.png)



- 0이하의 Input은 그대로 0을 출력하고, 0이상의 값은 그대로 출력한다.
- 큰 숫자를 통과시킬 수 있고 일부 뉴런을 무효화(0) 하여 해당 뉴런들은 신호를 출력하지 않는다. 
- 항상 신호를 출력하지 않기때문에 빠르고 적은 비용으로 훈련할 수 있다.

<br/>

### 인공신경망(ANN)

![img](https://i.stack.imgur.com/9jzpy.jpg)

- 퍼셉트론(perceptron)과 활성화함수(activation function)의 집합이다. 
- 훈련 시작 시 임의의 값으로 초기화 된 후 loss function을 이용해 실제 값과 대조함으로써 오류를 계산하고, 이 값을 기반으로 모든 단계에서의 Weight가 조정된다. 오류를 더이상 줄일 수 없을 때 훈련을 중지한다.

<br/>

### 신경망의 정규화 방법

- 정규화는 과적합(Overfitting)을 예방하고 성능을 높이는데에 도움을 준다.

#### - 드롭아웃(Dropout)

![img](https://cdn-images-1.medium.com/max/1044/1*iWQzxhVlvadk6VAJjsgXgg.png)

- Overfitting을 방지하기 위해 신경망을 정규화하는 효과적인 방법이다.
- 뉴런이 무작이로 훈련되며 Hidden layer의 unit을 무작위로 선택하기 때문에 매번 다른 모델 구조를 얻게 됩니다.

<br/>

#### - 배치 정규화 (Batch Nomalization)

![img](https://i0.wp.com/mohammadpz.github.io/posts/2015_02_01_IFT6266_Cats_vs_Dogs/img/bn.png)



- Layer의 Output을 평균0, 표준편차1로 정규화하는 것이다.
- Overfitting을 억제하고 신경망 모델의 학습 속도를 향상시켜 복잡한 모델을 학습시키는데 유용하다.

<br/>

#### - L1, L2 정규화 (L1 Regularization, L2 Regularization)

![img](https://qph.fs.quoracdn.net/main-qimg-c1d467e9062bf50fdaa38093bd24bd01)

- ##### L1 정규화

  - 가중치의 절댓값에 페널티를 부과하여 가중치를 0으로 만든다.

- ##### L2 정규화 

  - 가중치의 제곱값에 페널티를부과하여 훈련을 진행하는 중에 가중치의 크기를 작게 만든다.

- 두 정규화 방법 모두 가중치의 크기가 작을수록 더 좋은 모델이라고 평가한다.

<br/>

### 다중 클래스 분류

- 다중 클래스 분류 문제에서 사용되는 몇가지 용어를 살펴보려 한다.
- 아래 두가지를 조합하면 Neural Network에서 다중 클래스 분류를 할 수 있다. 

<br/>

#### - 원-핫 인코딩 (One-hot encoding)

![img](https://cdn-images-1.medium.com/max/1368/0*T5jaa2othYfXZX9W.)

- 분류문제에서 대상 변수 혹은 클래스를 나타내는 방법이다. 
- 대상 변수의 유사도는 고려하지 않는다.
- 인덱스 위치만 1이고 나머지는 모두 0으로 채워진 벡터이다. 100개의 클래스가 존재한다면 벡터크기는 100이며 1개의 1과 99개의 0으로 이루어져 있다.

<br/>

#### - 소프트맥스(Softmax)

![img](https://cdn-images-1.medium.com/max/800/1*gmOykUVXXUYK7LPDVZHMBg.png)



![img](https://qph.fs.quoracdn.net/main-qimg-fda2f008df90ed5d7b6aff89b881e1ac)



- k차원의 벡터 y를 (0,1)범위를 갖도록 치환한다고 볼 수 있다. 확률분포와 동일하며 Logistic regression이라고 할 수 있다. 
- Output의 총합을 1로 만들어주며, Output의 모든 값의 합으로 나누어 확률로 변환한다. 

<br/>

- **Sigmoid함수와 형태가 매우 유사한데 차이점이 뭘까?**
  - Sigmoid를 사용하면 Input과 Bias들에 의해 Output이 결정된다.
  - Softmax역시 Input과 Bias가 Output에 영향을 미치지만 전체뉴런들의 출력값과 비교하여 최종 Output이 결정된다. (= non=locality). 즉 모든 Output의 총합이 1이라는 성질을 이용해 해당 값이 나올 확률을 알 수 있고, 가장 큰 값이 나오는 쪽으로 분류가 된다는 것을 알 수 있다. 

<br/>

#### - 교차 엔트로피 (Cross Entropy)

![img](https://i.stack.imgur.com/GKdbq.png)

- 손실함수(Loss function)로 음의 로그 확률의 합이다. 오류를 최소화해야 한다는 특징이 있다. 

- Softmax와 One-hot encoding사이의 Output간의 거리를 비교한다. 
- 함수를 최대화 한다는 것은 동일함수의 음수를 최소화 하는 것과 동일하다.
- Cross Entropy의 원리를 쉽게 설명해 놓은 포스팅이 있어 [링크](http://blog.naver.com/PostView.nhn?blogId=gyrbsdl18&logNo=221013188633)를 걸어놓았다.